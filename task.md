# Домашнее задание №1
 
Первое задание по курсу ММОТ простое до безобразия.
Вам нужно обучить свою собственную маленькую LLM с нуля на непопулярном языке.
 
## Детали
 
* LLM должна быть обучена полностью вами, полностью с нуля.
* В качестве "непопулярного" языка возьмите любой, который не входит в топ-50 языков мира по числу носителей.
  Пример: **татарский** корпус можно взять здесь. 
  https://wortschatz.uni-leipzig.de/en/download/Tatar
  Это лишь пример - можете взять любой который вам нравится.
 
---
 
## Требования к модели
 
- Архитектура - decoder-only
- Количество слоёв и параметров - не регламентировано (но учтите, что модель должна генерировать адекватный текст, проверим гугл-переводчиком!)
- Длина контекста - не менее 256 символов
- Использование SFT и чат-темплейтов не обязательно, делаем простой претрейн
 
---
 
## Этапы работы
 
1. Сбор корпуса
   Найдите и соберите тексты на выбранном языке (100 МБ текстов может быть достаточно, но данных много не бывает).
   Очистите, подготовьте для обучения.
 
2. Обучение токенизатора
   Обучите свой токенизатор (например, SentencePiece или tokenizers от Hugging Face).
   Документация: https://huggingface.co/docs/tokenizers/python/latest/quicktour.html
 
3. Обучение модели
   Создайте и обучите небольшую GPT-модель (decoder-only).
   Можно использовать стандартные классы из `transformers`, но модель не должна быть предобученной.
   Полезные материалы для моральной поддержки:
 
   * Документация от Hugging Face: https://huggingface.co/learn/llm-course/en/chapter7/6
   * Репозиторий от Карпатого: https://github.com/karpathy/nanoGPT
 
4. Доказательство, что модель умеет говорить
   Придумайте тест, показывающий, что модель действительно обучилась.
   Например, пусть она отвечает на вопросы типа "Кто написал Евгения Онегина" на выбранном языке. Учтите, что ответа на один вопрос (который и так был в обучающем множестве) недостаточно. Покажите что модель отвечает одинаково при разных формулировках вопроса и т.д.
 
---
 
## Бонусы за эксперименты
 
Дополнительные баллы можно получить за:
 
* использование оптимизаций (например, Flash Attention 2, mixed precision, gradient checkpointing);
* эксперименты с архитектурой, размерами токенайзера;
* попытки применить продвинутые идеи — MoE, SWA, Rotary Embeddings и т.д.
 
 
---
 
## Что нужно сдать
 
1. Ссылку на репозиторий или ноутбук с полным кодом обучения (data / tokenizer / train).
2. Обученную модель (чекпоинт или Hugging Face Hub).
3. Отдельный ноутбук с демонстрацией инференса (примеры запросов и ответов).
 
---
 
## Критерии оценки (до 15 баллов)
 
| -------------------------------------- | --------- |
| Компонент                                    | Баллы   |
| -------------------------------------- | --------- |
| Подготовка корпуса                    | до 2 б   |
| Обучение токенизатора             | до 2 б    |
| Код и процесс обучения            | до 4 б  |
| Демонстрация работы модели | до 2 б  |
| Эксперименты                             | до 5 б  |
| -------------------------------------- | ------- |
 
---
 
## Примерные критерии оценивания
- Подготовка корпуса: 1 балл если просто взяли готовый, 2 балла если проанализировали его, посчитали статистики распределения слов и улучшили (почистили его от других языков)
- Обучение токенизатора: 1 балл если просто обучили токенизатор, 2 балла за эксперименты с размеров словаря, оценку получившегося распределения (низкочастотные токены и т.д.)
- Код и процесс обучения: будем обращать внимание на чистоту кода, использование ускорений и оптимизаций обучения, контроль процесса обучения (оценку лосса), использование для валидации few-shot примеров, логгирование в тензорборды и прочее
- Демонстрация работы модели - 1 балл если просто задали несколько вопросов и получили ответы, 2 балла если сделали что-то похожее на полноценный бенчмарк
- Эксперименты - Будем рады увидеть попытки использования нестандартных подходов, оптимизаций и архитектурных идей. Ничего конкретного не просим.
 
---
 
 
Удачи и вдохновения!
Если модель хоть немного научится говорить на вашем редком языке -- вы уже сделали маленькое чудо
 
 
---
 